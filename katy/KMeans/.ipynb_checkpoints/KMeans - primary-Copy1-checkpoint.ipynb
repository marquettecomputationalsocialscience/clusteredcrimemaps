{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    def __init__(self, k=8, euclid = True):\n",
    "        self.k = k\n",
    "        if (euclid):\n",
    "            self._distance = 'euclidean'\n",
    "        else:\n",
    "            self._distance = self._distance_haversine\n",
    "    \n",
    "    def _step(self):\n",
    "        \"\"\"Compute distance, assign groups, recompute centers\"\"\"\n",
    "        distance = cdist(self.X,self.cluster_centers,metric=self._distance)\n",
    "        self.labels = distance.argmin(1)\n",
    "       # centers = np.zeros((self.k,2))\n",
    "        for cluster in range(self.k):\n",
    "            points = self.X[self.labels == cluster]\n",
    "            if len(points) == 0:\n",
    "                distance = cdist(self.X,np.delete(self.cluster_centers,cluster,0),metric=self._distance)\n",
    "                mean_dist = np.mean(distance,0)\n",
    "                self.cluster_centers[cluster] = mean_dist.argmax()\n",
    "            else:\n",
    "                self.cluster_centers[cluster] = np.mean(points,0)\n",
    "       # self.cluster_centers = centers\n",
    "        \n",
    "    def _distance_haversine(self,a,b):\n",
    "        lat_1, lon_1, lat_2, lon_2 = map(np.radians,[a[0],a[1],b[0],b[1]])\n",
    "        d_lat = lat_2 - lat_1\n",
    "        d_lon = lon_2 - lon_1\n",
    "        \n",
    "        arc = np.sin(d_lat/2.0)**2 + np.cos(lat_1)*np.cos(lat_2)*np.sin(d_lon/2)**2\n",
    "        \n",
    "        c = 2 * np.arcsin(np.sqrt(arc))\n",
    "        km = 6372.8 * c\n",
    "        return km\n",
    "    \n",
    "    def _init_centers(self, X):\n",
    "        unique = np.unique(X, axis=0)\n",
    "        index = np.random.permutation(len(unique))[:self.k]\n",
    "        return unique[index]\n",
    "    \n",
    "    def fit(self,X, centers = None):\n",
    "        '''Expects centers to be inputted, if not random'''\n",
    "        self.labels = np.zeros(len(X))\n",
    "        self.X = X\n",
    "        if centers is not None:\n",
    "            self.cluster_centers = centers \n",
    "        else:\n",
    "            self.cluster_centers = self._init_centers(X)\n",
    "        old_centers = np.zeros((self.k,2))\n",
    "    #    self.i = 0\n",
    "        while(not np.array_equal(old_centers, self.cluster_centers)):\n",
    "            old_centers = self.cluster_centers.copy()\n",
    "            self._step()\n",
    "         #   self.i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "demographics = gpd.read_file('./census.geoJSON')\n",
    "\n",
    "def gen_coords(loc):\n",
    "    data = loc[1:-1].split(',')\n",
    "    data = list((np.float(data[0]), np.float(data[1])))\n",
    "    x.append(data[1])\n",
    "    y.append(data[0])\n",
    "    return [data[0],data[1]]\n",
    "\n",
    "def point_similarity(X,geo_labels, euc_labels,k):\n",
    "    '''For an inputted series of points, geodesic labels, euclidean labels, and k-value\n",
    "       returns the point-similarity index per geodesic cluster\n",
    "    '''\n",
    "\n",
    "    euc_cluster_totals = np.zeros(k,dtype=np.int)\n",
    "    geo_euc_composition = [np.zeros(k,dtype=np.int)* 1 for i in range(k)]\n",
    "    \n",
    "    for index,point in enumerate(geo_labels):\n",
    "        euc_cluster_totals[euc_labels[index]] += 1\n",
    "        geo_euc_composition[point][euc_labels[index]] += 1\n",
    "    \n",
    "    point_sim = []\n",
    "    for geo_cluster in range(k):\n",
    "        sim = 0\n",
    "        for euc_cluster in range(k):\n",
    "            matching_points = geo_euc_composition[geo_cluster][euc_cluster]\n",
    "            euc_percentage = matching_points / euc_cluster_totals[euc_cluster]\n",
    "            geo_percentage = matching_points / np.sum(geo_euc_composition[geo_cluster])\n",
    "            sim += euc_percentage * geo_percentage\n",
    "        point_sim.append(sim)\n",
    "\n",
    "    return np.array(point_sim)\n",
    "\n",
    "def minority_probability(X,cluster_number,geo_labels,demographics):\n",
    "        points = X[geo_labels == cluster_number]\n",
    "        # geoJSON puts points in Long/Lat order\n",
    "        # but points are in lat/long earlier\n",
    "        hull = shapely.geometry.multipoint.MultiPoint([[p[1],p[0]] for p in points]).convex_hull\n",
    "  \n",
    "        pop = np.zeros(7)\n",
    "        for index in range(len(demographics)):\n",
    "            census_tract = demographics.loc[index,'geometry']\n",
    "            intersect = hull.intersection(census_tract)\n",
    "            overlap = intersect.area/census_tract.area\n",
    "            if (overlap != 0):\n",
    "                pop = pop + (np.array(demographics.loc[index,['White','Black or African American', 'American Indian and Ala Native',\n",
    "                   'Asian','Native Hawaiian/other Pac Isl', 'Multiple Race',\n",
    "                   'Other Race']]) * overlap)\n",
    "        \n",
    "        if (np.all(pop ==0)):\n",
    "            return 0\n",
    "        \n",
    "        return (pop[1:]/np.sum(pop)).sum()\n",
    "\n",
    "def bias_index(X, geo_labels, euc_labels, demographics, k):\n",
    "    if np.all(geo_labels == euc_labels):\n",
    "        return 0\n",
    "\n",
    "    dissimilarity_index = 1 - point_similarity(X,geo_labels,euc_labels,k)\n",
    "    minority_prob = np.array([minority_probability(X,cluster,geo_labels,demographics) \n",
    "                              for cluster in range(k)])\n",
    "    \n",
    "    potential_bias = minority_prob * dissimilarity_index\n",
    "    return potential_bias.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['year','k','t_jan','t_feb','t_mar','t_april', 't_may', 't_june', 't_july',\n",
    "          't_aug', 't_sep','t_oct', 't_nov', 't_dec','m_jan','m_feb','m_mar','m_april', \n",
    "          'm_may', 'm_june', 'm_july', 'm_aug', 'm_sep','m_oct', 'm_nov', 'm_dec']\n",
    "\n",
    "\n",
    "\n",
    "frame_list = []\n",
    "for year in range(2005,2017):\n",
    "    for k in range(2,11):\n",
    "        year_list = [str(year)]\n",
    "        year_list.append(k)\n",
    "        for _ in range(24):\n",
    "            year_list.append(0.00) \n",
    "        frame_list.append(year_list)\n",
    "    \n",
    "bias_frame = pd.DataFrame(data=frame_list, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_bias(folder,file,bias_value,k):\n",
    "    year_string = folder.split('data_')[1]\n",
    "    year_array = np.array(bias_frame.year == year_string) \n",
    "    k_array = np.array(bias_frame.k == k)\n",
    "    index = np.logical_and(year_array,k_array)\n",
    "    month_string = file.split('.csv')[0]\n",
    "    prefix,month = month_string.split('theft_')\n",
    "    if len(prefix) == 0:\n",
    "        month_index = 't_' + month\n",
    "    else:\n",
    "        month_index = prefix + month\n",
    "    bias_frame.loc[index,month_index] = bias_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datamound/data_2005-m_theft_may.js\n",
      "./datamound/data_2005-m_theft_dec.js\n",
      "./datamound/data_2005-theft_nov.js\n",
      "./datamound/data_2005-theft_dec.js\n",
      "./datamound/data_2005-theft_may.js\n",
      "./datamound/data_2005-m_theft_sep.js\n",
      "./datamound/data_2005-m_theft_april.js\n",
      "./datamound/data_2005-theft_mar.js\n",
      "./datamound/data_2005-m_theft_oct.js\n",
      "./datamound/data_2005-theft_july.js\n",
      "./datamound/data_2005-theft_jan.js\n",
      "./datamound/data_2005-m_theft_june.js\n",
      "./datamound/data_2005-m_theft_feb.js\n",
      "./datamound/data_2005-theft_oct.js\n",
      "./datamound/data_2005-m_theft_july.js\n",
      "./datamound/data_2005-theft_aug.js\n",
      "./datamound/data_2005-theft_feb.js\n",
      "./datamound/data_2005-theft_april.js\n",
      "./datamound/data_2005-m_theft_aug.js\n",
      "./datamound/data_2005-m_theft_mar.js\n",
      "./datamound/data_2005-m_theft_nov.js\n",
      "./datamound/data_2005-m_theft_jan.js\n",
      "./datamound/data_2005-theft_june.js\n",
      "./datamound/data_2005-theft_sep.js\n",
      "./datamound/data_2006-m_theft_may.js\n",
      "./datamound/data_2006-m_theft_dec.js\n",
      "./datamound/data_2006-theft_nov.js\n",
      "./datamound/data_2006-theft_dec.js\n",
      "./datamound/data_2006-theft_may.js\n",
      "./datamound/data_2006-m_theft_sep.js\n",
      "./datamound/data_2006-m_theft_april.js\n",
      "./datamound/data_2006-theft_mar.js\n",
      "./datamound/data_2006-m_theft_oct.js\n",
      "./datamound/data_2006-theft_july.js\n",
      "./datamound/data_2006-theft_jan.js\n",
      "./datamound/data_2006-m_theft_june.js\n",
      "./datamound/data_2006-m_theft_feb.js\n",
      "./datamound/data_2006-theft_oct.js\n",
      "./datamound/data_2006-m_theft_july.js\n",
      "./datamound/data_2006-theft_aug.js\n",
      "./datamound/data_2006-theft_feb.js\n",
      "./datamound/data_2006-theft_april.js\n",
      "./datamound/data_2006-m_theft_aug.js\n",
      "./datamound/data_2006-m_theft_mar.js\n",
      "./datamound/data_2006-m_theft_nov.js\n",
      "./datamound/data_2006-m_theft_jan.js\n",
      "./datamound/data_2006-theft_june.js\n",
      "./datamound/data_2006-theft_sep.js\n",
      "./datamound/data_2007-m_theft_may.js\n",
      "./datamound/data_2007-m_theft_dec.js\n",
      "./datamound/data_2007-theft_nov.js\n",
      "./datamound/data_2007-theft_dec.js\n",
      "./datamound/data_2007-theft_may.js\n",
      "./datamound/data_2007-m_theft_sep.js\n",
      "./datamound/data_2007-m_theft_april.js\n",
      "./datamound/data_2007-theft_mar.js\n",
      "./datamound/data_2007-m_theft_oct.js\n",
      "./datamound/data_2007-theft_july.js\n",
      "./datamound/data_2007-theft_jan.js\n",
      "./datamound/data_2007-m_theft_june.js\n",
      "./datamound/data_2007-m_theft_feb.js\n",
      "./datamound/data_2007-theft_oct.js\n",
      "./datamound/data_2007-m_theft_july.js\n",
      "./datamound/data_2007-theft_aug.js\n",
      "./datamound/data_2007-theft_feb.js\n",
      "./datamound/data_2007-theft_april.js\n",
      "./datamound/data_2007-m_theft_aug.js\n",
      "./datamound/data_2007-m_theft_mar.js\n",
      "./datamound/data_2007-m_theft_nov.js\n",
      "./datamound/data_2007-m_theft_jan.js\n",
      "./datamound/data_2007-theft_june.js\n",
      "./datamound/data_2007-theft_sep.js\n",
      "./datamound/data_2008-m_theft_may.js\n",
      "./datamound/data_2008-m_theft_dec.js\n",
      "./datamound/data_2008-theft_nov.js\n",
      "./datamound/data_2008-theft_dec.js\n",
      "./datamound/data_2008-theft_may.js\n",
      "./datamound/data_2008-m_theft_sep.js\n",
      "./datamound/data_2008-m_theft_april.js\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def gen_coords(loc):\n",
    "    data = loc[1:-1].split(',')\n",
    "    data = list((np.float(data[0]), np.float(data[1])))\n",
    "    x.append(data[1])\n",
    "    y.append(data[0])\n",
    "    return [data[0],data[1]]\n",
    "\n",
    "def percent_similarity(a,b):\n",
    "    return len(a[a==b])/len(a)\n",
    "\n",
    "for year in range(2005,2017):\n",
    "    folder = 'data_' + str(year)\n",
    "    for file in os.listdir('../data/' + folder):\n",
    "        if(file.endswith('.csv')):\n",
    "            df = pd.read_csv('../data/' + folder +'/' + file, sep =';')\n",
    "        \n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            df['Points'] = df['Location'].apply(gen_coords)\n",
    "            points = [Point(xy) for xy in zip(x,y)]\n",
    "            crs = {'init': 'epsg:4326'}\n",
    "            geo_df = GeoDataFrame(df,crs=crs, geometry=points)\n",
    "            theft_both = geo_df.copy()\n",
    "            test_list = []\n",
    "\n",
    "            for index in range(len(theft_both)):\n",
    "                test_list.append(df.loc[index, 'Points'])\n",
    "\n",
    "            X = np.array(test_list)\n",
    "\n",
    "            for k in range(2,11):\n",
    "                euclid = KMeans(k = k)\n",
    "                geodesic = KMeans(k = k, euclid = False)\n",
    "                centers = geodesic._init_centers(X)\n",
    "\n",
    "                euclid.fit(X,centers = centers)\n",
    "                geodesic.fit(X,centers = centers) \n",
    "\n",
    "                bias_val = bias_index(X, geodesic.labels, euclid.labels, demographics, k)\n",
    "              #  print(folder,file,bias_val,k)\n",
    "                store_bias(folder,file,bias_val,k)\n",
    "                theft_both.loc[:,'e_cluster' + 'K' + str(k)] = euclid.labels.copy()\n",
    "                theft_both.loc[:,'g_cluster' + 'K' + str(k)] = geodesic.labels.copy()\n",
    "                \n",
    "\n",
    "\n",
    "              #  print(percent_similarity(euclid.labels, geodesic.labels))\n",
    "\n",
    "\n",
    "\n",
    "            theft_both = theft_both.drop('Points', axis=1)\n",
    "\n",
    "            try:\n",
    "                os.remove('./datamound/'+ folder + '-' + file.split('.csv')[0] + '.js')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "            theft_both.to_file('./datamound/'+ folder + '-' +file.split('.csv')[0] + '.js', driver='GeoJSON')\n",
    "    #         with open('./datamound/'+file.split('.csv')[0] + '.js', 'r') as original: data = original.read()\n",
    "    #         with open('./datamound/'+file.split('.csv')[0] + '.js', 'w') as modified: modified.write('var both =' \n",
    "    #                                                         + data +';')\n",
    "            print('./datamound/'+ folder + '-' +file.split('.csv')[0] + '.js')\n",
    "          #  print('-------')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data = 'var dataB = ['\n",
    "for year in range(2011,2017):\n",
    "    \n",
    "    ordered_names = ['theft_jan.js','theft_feb.js','theft_mar.js','theft_april.js',\n",
    "                'theft_may.js','theft_june.js','theft_july.js','theft_aug.js',\n",
    "                'theft_sep.js','theft_oct.js','theft_nov.js','theft_dec.js',\n",
    "                'm_theft_jan.js','m_theft_feb.js','m_theft_mar.js','m_theft_april.js',\n",
    "                'm_theft_may.js','m_theft_june.js','m_theft_july.js','m_theft_aug.js',\n",
    "                'm_theft_sep.js','m_theft_oct.js','m_theft_nov.js','m_theft_dec.js']\n",
    "    year_string = 'data_'+str(year)+'-'\n",
    "\n",
    "    for file in ordered_names:\n",
    "        reader = open('./datamound/'+ year_string + file,'r')\n",
    "        data += (reader.read() + ',')\n",
    "        reader.close()\n",
    "        print(file)\n",
    "        \n",
    "    writer = open('halfB.js','w')\n",
    "    writer.write(data + '];')\n",
    "    writer.close()\n",
    "\n",
    "    \n",
    "data = 'var dataA = ['\n",
    "for year in range(2005,2011):\n",
    "\n",
    "    ordered_names = ['theft_jan.js','theft_feb.js','theft_mar.js','theft_april.js',\n",
    "                'theft_may.js','theft_june.js','theft_july.js','theft_aug.js',\n",
    "                'theft_sep.js','theft_oct.js','theft_nov.js','theft_dec.js',\n",
    "                'm_theft_jan.js','m_theft_feb.js','m_theft_mar.js','m_theft_april.js',\n",
    "                'm_theft_may.js','m_theft_june.js','m_theft_july.js','m_theft_aug.js',\n",
    "                'm_theft_sep.js','m_theft_oct.js','m_theft_nov.js','m_theft_dec.js']\n",
    "    year_string = 'data_'+str(year)+'-'\n",
    "\n",
    "    for file in ordered_names:\n",
    "        reader = open('./datamound/'+ year_string + file,'r')\n",
    "        data += (reader.read() + ',')\n",
    "        reader.close()\n",
    "        print(file)\n",
    "\n",
    "    writer = open('halfA.js','w')\n",
    "    writer.write(data + '];')\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_frame.to_json('bias.js')\n",
    "with open('bias.js','r') as reader:\n",
    "    data = reader.read()\n",
    "with open('bias.js','w') as w:\n",
    "    w.write('var bias_data =' + data + ';')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
